{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "实用方法(工具)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\"\"\"\n",
    "此例子中用到的数据是从 Tomas Mikolov 的网站取得的 PTB 数据集\n",
    "PTB 文本数据集是语言模型学习中目前最广泛的数据集。\n",
    "数据集中我们只需要利用 data 文件夹中的\n",
    "ptb.test.txt，ptb.train.txt，ptb.valid.txt 三个数据文件\n",
    "测试，训练，验证 数据集\n",
    "这三个数据文件是已经经过预处理的，包含10000个不同的词语和语句结束标识符 <eos> 的\n",
    "\n",
    "要获得此数据集，只需要用下面一行命令：\n",
    "wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "\n",
    "如果没有 wget 的话，就安装一下：\n",
    "sudo apt install wget\n",
    "\n",
    "解压下载下来的压缩文件：\n",
    "tar xvf simple-examples.tgz\n",
    "\n",
    "==== 一些术语的概念 ====\n",
    "# Batch size : 批次(样本)数目。一次迭代（Forword 运算（用于得到损失函数）以及 BackPropagation 运算（用于更新神经网络参数））所用的样本数目。Batch size 越大，所需的内存就越大\n",
    "# Iteration : 迭代。每一次迭代更新一次权重（网络参数），每一次权重更新需要 Batch size 个数据进行 Forward 运算，再进行 BP 运算\n",
    "# Epoch : 纪元/时代。所有的训练样本完成一次迭代\n",
    "\n",
    "# 假如 : 训练集有 1000 个样本，Batch_size=10\n",
    "# 那么 : 训练完整个样本集需要： 100 次 Iteration，1 个 Epoch\n",
    "# 但一般我们都不止训练一个 Epoch\n",
    "\n",
    "==== 超参数（Hyper parameter）====\n",
    "init_scale : 权重参数（Weights）的初始取值跨度，一开始取小一些比较利于训练\n",
    "learning_rate : 学习率，训练时初始为 1.0\n",
    "num_layers : LSTM 层的数目（默认是 2）\n",
    "num_steps : LSTM 展开的步（step）数，相当于每个批次输入单词的数目（默认是 35）\n",
    "hidden_size : LSTM 层的神经元数目，也是词向量的维度（默认是 650）\n",
    "max_lr_epoch : 用初始学习率训练的 Epoch 数目（默认是 10）\n",
    "dropout : 在 Dropout 层的留存率（默认是 0.5）\n",
    "lr_decay : 在过了 max_lr_epoch 之后每一个 Epoch 的学习率的衰减率，训练时初始为 0.93。让学习率逐渐衰减是提高训练效率的有效方法\n",
    "batch_size : 批次(样本)数目。一次迭代（Forword 运算（用于得到损失函数）以及 BackPropagation 运算（用于更新神经网络参数））所用的样本数目\n",
    "（batch_size 默认是 20。取比较小的 batch_size 更有利于 Stochastic Gradient Descent（随机梯度下降），防止被困在局部最小值）\n",
    "\"\"\"\n",
    "\n",
    "# 数据集的目录\n",
    "data_path = \"data\"\n",
    "\n",
    "# 保存训练所得的模型参数文件的目录\n",
    "save_path = './save'\n",
    "\n",
    "# 测试时读取模型参数文件的名称\n",
    "load_file = \"train-checkpoint-69\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# 数据集的目录\n",
    "parser.add_argument('--data_path', type=str, default=data_path, help='The path of the data for training and testing')\n",
    "# 测试时读取模型参数文件的名称\n",
    "parser.add_argument('--load_file', type=str, default=load_file, help='The path of checkpoint file of model variables saved during training')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 如果是 Python3 版本\n",
    "Py3 = sys.version_info[0] == 3\n",
    "\n",
    "\n",
    "# 将文件根据句末分割符 <eos> 来分割\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        if Py3:\n",
    "            return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "        else:\n",
    "            return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "# 构造从单词到唯一整数值的映射\n",
    "# 后面的其他数的整数值按照它们在数据集里出现的次数多少来排序，出现较多的排前面\n",
    "# 单词 the 出现频次最多，对应整数值是 0\n",
    "# <unk> 表示 unknown（未知），第二多，整数值为 1\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    # 用 Counter 统计单词出现的次数，为了之后按单词出现次数的多少来排序\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "\n",
    "    # 单词到整数的映射\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "\n",
    "# 将文件里的单词都替换成独一的整数\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "# 加载所有数据，读取所有单词，把其转成唯一对应的整数值\n",
    "def load_data(data_path):\n",
    "    # 确保包含所有数据集文件的 data_path 文件夹在所有 Python 文件\n",
    "    # 的同级目录下。当然了，你也可以自定义文件夹名和路径\n",
    "    if not os.path.exists(data_path):\n",
    "        raise Exception(\"包含所有数据集文件的 {} 文件夹 不在此目录下，请添加\".format(data_path))\n",
    "\n",
    "    # 三个数据集的路径\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    # 建立词汇表，将所有单词（word）转为唯一对应的整数值（id）\n",
    "    word_to_id = build_vocab(train_path)\n",
    "\n",
    "    # 训练，验证和测试数据\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "\n",
    "    # 所有不重复单词的个数\n",
    "    vocab_size = len(word_to_id)\n",
    "\n",
    "    # 反转一个词汇表：为了之后从 整数 转为 单词\n",
    "    id_to_word = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    print(word_to_id)\n",
    "    print(\"===================\")\n",
    "    print(vocab_size)\n",
    "    print(\"===================\")\n",
    "    print(train_data[:10])\n",
    "    print(\"===================\")\n",
    "    print(\" \".join([id_to_word[x] for x in train_data[:10]]))\n",
    "    print(\"===================\")\n",
    "    return train_data, valid_data, test_data, vocab_size, id_to_word\n",
    "\n",
    "\n",
    "# 生成批次样本\n",
    "def generate_batches(raw_data, batch_size, num_steps):\n",
    "    # 将数据转为 Tensor 类型\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    # 将数据形状转为 [batch_size, batch_len]\n",
    "    data = tf.reshape(raw_data[0: batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    # range_input_producer 可以用多线程异步的方式从数据集里提取数据\n",
    "    # 用多线程可以加快训练，因为 feed_dict 的赋值方式效率不高\n",
    "    # shuffle 为 False 表示不打乱数据而按照队列先进先出的方式提取数据\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "\n",
    "    # 假设一句话是这样： “我爱我的祖国和人民”\n",
    "    # 那么，如果 x 是类似这样： “我爱我的祖国”\n",
    "    x = data[:, i * num_steps:(i + 1) * num_steps]\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    # y 就是类似这样（正好是 x 的时间步长 + 1）： “爱我的祖国和”\n",
    "    # 因为我们的模型就是要预测一句话中每一个单词的下一个单词\n",
    "    # 当然这边的例子很简单，实际的数据不止一个维度\n",
    "    y = data[:, i * num_steps + 1: (i + 1) * num_steps + 1]\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# 输入数据\n",
    "class Input(object):\n",
    "    def __init__(self, batch_size, num_steps, data):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.epoch_size = ((len(data) // batch_size) - 1) // num_steps\n",
    "        # input_data 是输入，targets 是期望的输出\n",
    "        self.input_data, self.targets = generate_batches(data, batch_size, num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络模型\n",
    "import tensorflow as tf\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, input, is_training, hidden_size, vocab_size, num_layers, dropout=0.5, init_scale=0.05):\n",
    "        self.is_training = is_training\n",
    "        self.input_obj = input\n",
    "        self.batch_size = input.batch_size\n",
    "        self.num_steps = input.num_steps\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size, self.hidden_size], -init_scale, init_scale))\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self.input_obj.input_data)\n",
    "            \n",
    "        if is_training and dropout < 1:\n",
    "            inputs = tf.nn.dropout(inputs, dropout)\n",
    "            \n",
    "        \n",
    "        self.init_state = tf.placeholder(tf.float32, [num_layersyers, 2, self.batch_size, self.hidden_size])\n",
    "        \n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "        \n",
    "        rnn_tuple_state = tuple(\n",
    "            [tf.contrib.rnn.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])]\n",
    "            for idx in range(num_layers))\n",
    "        \n",
    "        cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "        \n",
    "        if is_training and dropout < 1:\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob = dropout)\n",
    "            \n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell for _ in range(num_layers)], state_is_tuple=True)\n",
    "            \n",
    "        output, self.state = tf.rnn.dynamic_rnn(cell, inputs, dtype=tf.float32, init_state=rnn_tuple_state)\n",
    "        \n",
    "        output = tf.reshape(output, [-1, hidden_size])\n",
    "        \n",
    "        softmax_w = tf.Variable.random_uniform([hidden_size, vocab_size], -init_scale, init_scale)\n",
    "        softmax_b = tf.Variable.random_uniform([vocab_size], -init_scale, init_scale)\n",
    "        \n",
    "        logits = tf.nn.wx_plus_b(output, softmax_w, softmax_b)\n",
    "        logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])\n",
    "        \n",
    "        loss = tf.contrib.seq2seq.sequence_loss(logits, self.input_obj.targets, tf.ones([self.batch_size, self.num_steps],dtype=tf.float32), \n",
    "                                                average_across_timesteps = False, average_across_barch = True)\n",
    "        \n",
    "        self.cost = tf.reduce_sum(loss)\n",
    "        \n",
    "        self.softmax_out = tf.nn.softmax(tf.reshape(logits, [-1, vocab_size]))\n",
    "        \n",
    "        self.predict = tf.cast(tf.argmax(self.softmax_out, axis=1), tf.int32)\n",
    "        \n",
    "        correct_prediction = tf.equal(self.predict, tf.reshape(self.input_obj.targets, [-1]))\n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        if not is_training:\n",
    "            return\n",
    "        \n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),5)\n",
    "        \n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        \n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=tf.get_or_create_global_step())\n",
    "        \n",
    "        self.new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "        self.lr_update = tf.assign(tf.learning_rate, self.new_lr)\n",
    "        def assign_lr(self, session, lr_value):\n",
    "            session.run(self.lr_update, feed_dict={self.new_lr: lr_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "from util import *\n",
    "from  netword import *\n",
    "import tensorflow as tf\n",
    "\n",
    "def train(train_data, vocab_size, num_layers, num_epochs, batch_size, model_save_name,learning_rate=1.0,lr_decay=0.93,print_interation=50):\n",
    "    training_input = Input(batch_size=batch_size, num_steps=35,data=train_data)\n",
    "    m = Model(training_input, is_training=True,hidden_size=650,vocab_size=vocab_size,num_layers=num_layers)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    orig_decay = lr_decay\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_quene_runners(coord=coord)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            new_lr_decay = orig_decay ** max(epoch + 1 - max_lr_epoch, 0.0)\n",
    "            m.assign_lr(sess, learning_rate * new_lr_decay)\n",
    "            current_state = np.zeros((num_layers,2,batch_size,m.hidden_size))\n",
    "            curr_time = datetime.datetime.now()\n",
    "            \n",
    "            for step in range(training_input.epoch_size):\n",
    "                if step % print_interation != 0:\n",
    "                    cost,_,current_state = sess.run([m.cost, m.train_op, m.state], feed_dict={m.init_state:current_state})\n",
    "                else:\n",
    "                    seconds = (float(datetime.datetime.now() - curr_time).seconds) / print_interation\n",
    "                    curr_time = datetime.datetime.now()\n",
    "                    cost,_,current_state,acc = sees.run([m.cost,m.train_op,m.state,m.accuracy],feed_dict={m.init_state:current_state})\n",
    "                    print(\"Epoch {} Step {} Cost:{:3f} Accuracy:{} Seconds for steo{:3f}\".format(epoch,step,cost,accuracy,seconds))\n",
    "                    \n",
    "            saver.save(sess,save_path + '/' + model_save_name, global_step=epoch)\n",
    "        \n",
    "        saver.save(sess,save_path + '/' + model_save_name + '-final')\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def test(model_path, test_data, vocab_size, id_to_word):\n",
    "    test_input  = Input(batch_size = 20, num_steps=35,data=test_data)\n",
    "    \n",
    "    m = Model(test_input, is_training=false,hidden_size=650,vocab_size=vocab_size,num_layers=2)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_quene_runners(corrd=coord)\n",
    "        current_state = np.zeros((m.num_layers,2,m.batch_size,m.hidden_size))\n",
    "        \n",
    "        saver.restore(sess, model_path)\n",
    "        \n",
    "        num_acc_batches = 30\n",
    "        \n",
    "        check_batch_idx = 25\n",
    "        \n",
    "        acc_check_thresh = 5\n",
    "        \n",
    "        accuracy = 0\n",
    "        \n",
    "        for batch in range(num_acc_batches):\n",
    "            if batch == check_batch_idx:\n",
    "                true, pred, current_state, acc = sess.run([m.input_obj.targets, m.predict, m.state, m.accuracy], feed_dict={m.init_state:current_state})\n",
    "                pred_words = [id_to_word[x] for x in pred[:m.num_steps]]\n",
    "                true_words = [id_to_word[x] for x in true[0]]\n",
    "                \n",
    "            else:\n",
    "                acc, current_state = sess.run([m.accuracy, m.state], feed_dict={m.init_state:current_state})\n",
    "            \n",
    "            if batch >= acc_check_thresh:\n",
    "                accuracy += acc\n",
    "                \n",
    "        coord.request.stop()\n",
    "        coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
